{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION TO EXTRACT THE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "def extract_body_keypoints(frame):\n",
    "    with mp_pose.Pose(static_image_mode=True) as pose:\n",
    "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if results.pose_landmarks is None:\n",
    "            return None\n",
    "        keypoints = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append((landmark.x, landmark.y))\n",
    "        return keypoints\n",
    "\n",
    "def extract_features(keypoints):\n",
    "    left_shoulder_index = 11\n",
    "    left_elbow_index = 13\n",
    "    left_hip_index = 23\n",
    "    left_knee_index = 25\n",
    "\n",
    "    right_shoulder_index = 12\n",
    "    right_elbow_index = 14\n",
    "    right_hip_index = 24\n",
    "    right_knee_index = 26\n",
    "\n",
    "    # Extract the coordinates of the relevant keypoints\n",
    "    left_shoulder = keypoints[left_shoulder_index]\n",
    "    left_elbow = keypoints[left_elbow_index]\n",
    "    left_hip = keypoints[left_hip_index]\n",
    "    left_knee = keypoints[left_knee_index]\n",
    "\n",
    "    right_shoulder = keypoints[right_shoulder_index]\n",
    "    right_elbow = keypoints[right_elbow_index]\n",
    "    right_hip = keypoints[right_hip_index]\n",
    "    right_knee = keypoints[right_knee_index]\n",
    "\n",
    "    # Compute the angles using vector operations\n",
    "    left_shoulder_elbow = np.array(left_elbow) - np.array(left_shoulder)\n",
    "    left_hip_knee = np.array(left_knee) - np.array(left_hip)\n",
    "\n",
    "    right_shoulder_elbow = np.array(right_elbow) - np.array(right_shoulder)\n",
    "    right_hip_knee = np.array(right_knee) - np.array(right_hip)\n",
    "\n",
    "    left_shoulder_angle = np.arctan2(left_shoulder_elbow[1], left_shoulder_elbow[0])\n",
    "    left_hip_angle = np.arctan2(left_hip_knee[1], left_hip_knee[0])\n",
    "\n",
    "    right_shoulder_angle = np.arctan2(right_shoulder_elbow[1], right_shoulder_elbow[0])\n",
    "    right_hip_angle = np.arctan2(right_hip_knee[1], right_hip_knee[0])\n",
    "\n",
    "    # Convert angles from radians to degrees\n",
    "    left_shoulder_angle_deg = np.degrees(left_shoulder_angle)\n",
    "    left_hip_angle_deg = np.degrees(left_hip_angle)\n",
    "\n",
    "    right_shoulder_angle_deg = np.degrees(right_shoulder_angle)\n",
    "    right_hip_angle_deg = np.degrees(right_hip_angle)\n",
    "\n",
    "    # Combine the extracted features into a single array\n",
    "    features = np.array([left_shoulder_angle_deg, left_hip_angle_deg, right_shoulder_angle_deg, right_hip_angle_deg])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATH TO THE TRAIN AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "\n",
    "actions = [\"running\", \"jumping\"]  \n",
    "\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READING THE VIDEOS AND CONVERTING THE FEATURES TO A NUMPY ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action_idx, action in enumerate(actions):\n",
    "    action_path = os.path.join(train_dir, action)\n",
    "    for video_file in os.listdir(action_path):\n",
    "        video_path = os.path.join(action_path, video_file)\n",
    "        video_capture = cv2.VideoCapture(video_path)\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            keypoints = extract_body_keypoints(frame)\n",
    "            if keypoints is not None:\n",
    "                features = extract_features(keypoints)\n",
    "                X.append(features)\n",
    "                y.append(action_idx)\n",
    "        \n",
    "        video_capture.release()\n",
    "\n",
    "# Convert X and y to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITTING THE DATA INTO TRAINING AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE K-NN CLASSIFIER MODEL IN THE TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)  \n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_action_classifier.sav']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"knn_action_classifier.sav\"\n",
    "joblib.dump(knn_classifier, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_knn_classifier = joblib.load('knn_action_classifier.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction: jumping\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the test video\n",
    "test_video_path = \"test/test.mp4\"\n",
    "video_capture = cv2.VideoCapture(test_video_path)\n",
    "\n",
    "# Lists to store the extracted features and predicted actions for each frame\n",
    "test_features = []\n",
    "predicted_actions = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    keypoints = extract_body_keypoints(frame)\n",
    "    if keypoints is not None:\n",
    "        features = extract_features(keypoints)\n",
    "        test_features.append(features)\n",
    "\n",
    "# Convert the list of features to a numpy array\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_knn_classifier.predict(test_features)\n",
    "\n",
    "# Map the predicted labels to action names\n",
    "action_names = [\"running\", \"jumping\"]  # Add more actions if needed\n",
    "predicted_actions = [action_names[prediction] for prediction in predictions]\n",
    "\n",
    "# # Print the predicted actions for each frame\n",
    "# for i, action in enumerate(predicted_actions):\n",
    "#     print(f\"Frame {i + 1}: {action}\")\n",
    "\n",
    "\n",
    "final_prediction = max(set(predicted_actions), key=predicted_actions.count)\n",
    "# Print the final prediction for the entire video\n",
    "print(\"Final Prediction:\", final_prediction)\n",
    "\n",
    "# Release the video capture\n",
    "video_capture.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
